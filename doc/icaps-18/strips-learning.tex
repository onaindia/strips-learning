\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai17}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{}
\setcounter{secnumdepth}{0}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{comment}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{multicol}
\usepackage{arydshln}

\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}


\begin{document}

\title{Learning STRIPS action models with classical planning}

\author{Diego Garc\'ia\and Sergio Jim\'enez\and Eva Onaindia\\
{\small Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\small Universitat Polit\'ecnica de Val\'encia.}\\
{\small Camino de Vera s/n. 46022 Valencia, Spain}\\
{\small \{,serjice,onaindia\}@dsic.upv.es}}
 
\maketitle
\begin{abstract}
This paper presents a novel approach for learning STRIPS action models from examples that compiles the inductive learning task into a classical planning task. Our compilation for this learning task is flexible to different amounts of available input knowledge; it accepts partially specified action models and what is more, the input learning examples can range from a set of plans (with their corresponding initial and final states) to only a set of initial and final states where no action is observed.   
\end{abstract}


\section{Introduction}
Besides plan synthesis, planning action models are also useful for plan/goal recognition~\cite{ramirez2010probabilistic}. In both tasks, off-the-shelf planners require reasoning about action models that correctly and completely capture the possible world transitions~\cite{ghallab2004automated,geffner:book:2013}. Unfortunately, building such planning action models is complex, even for planning experts, so this knowledge acquisition bottleneck limits the potential of automated planning~\cite{kambhampati:modellite:AAAI2007}.  

On the other hand, Machine Learning (ML) techniques are able to compute a wide range of different kinds of models from examples~\cite{michalski2013machine}. However, the application of inductive ML techniques for learning planning action models is not straightforward. First, the inputs to ML algorithms usually are sets of finite numeric vectors encoding the value of sets of objects features. The input for learning planning action models traditionally are sets of observations of plan executions (each with possibly different length). Second, the traditional output of off-the-shelf ML techniques is a scalar value (an integer, in the case of classification tasks, or a real value, in the case of regression tasks). In the case of learning STRIPS action models, the output is not a scalar but a model of the preconditions and the effects of each action that defines the possible state transitions in the given planning domain. 

Learning STRIPS action models is a well-studied problem with sophisticated algorithms, like {\sc ARMS}~\cite{yang2007learning}, {\sc SLAF}~\cite{amir:alearning:JAIR08} or {\sc LOCM}~\cite{cresswell2013acquiring} that do not require full knowledge of all the states traversed by the example plans. Motivated by recent advances on learning generative models with classical planning~\cite{bonet2009automatic,segovia2016hierarchical,segovia2017generating} this paper introduces an innovative approach for learning classical planning action models that (1) it can be defined as a classical planning compilation and (2), it is flexible to different amounts of available input knowledge.


\section{Background}
This section defines the planning models used on this work.

\subsection{Classical planning}
We use $F$ to denote the set of {\em fluents} (propositional variables) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$, i.e.~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (WLOG we assume that $L$ does not assign conflicting values to any fluent). We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$, i.e.~all partial assignments of values to fluents. A {\em state} $s$ is then a total assignment of values to fluents, i.e. $|s|=|F|$, so the size of the state space $2^{|F|}$. Explicitly including negative literals $\neg f$ in states simplifies subsequent definitions, but we often abuse notation by defining a state $s$ only in terms of the fluents that are true in $s$, as is common in \strips\ planning.

A {\em classical planning frame} is a tuple $\Phi=\tup{F,A}$, where $F$ is a set of fluents and $A$ is a set of actions. Each action $a\in A$ has a set of literals $\pre(a)\in\mathcal{L}(F)$, called {\em preconditions}, a set of positive effects $\eff^+(a)\in\mathcal{L}(F)$, and a set of negative effects $\eff^-(a)\in\mathcal{L}(F)$. An action $a\in A$ is applicable in state $s$ iff $\pre(a)\subseteq s$, and the result of applying $a$ in $s$ is a new state $\theta(s,a)=(s\setminus \eff^-(a))\cup\eff^+(a)$.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is an initial state and $G\in\mathcal{L}(F)$ is a goal condition. A {\em plan} for $P$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$ that induces a state sequence $\tup{s_0, s_1, \ldots, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, $a_i$ is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. The plan $\pi$ {\em solves} $P$ if and only if $G\subseteq s_n$, i.e.~if the goal condition is satisfied following the application of $\pi$ in $I$.

In this work we assume that the fluents in $F$ are instantiated from predicates, as in PDDL~\cite{mcdermott1998pddl,fox2003pddl2}. There exists a set of predicates $\Psi$, each $p\in\Psi$ with an argument list of arity $ar(p)$. Given a set of objects $\Omega$, the set of fluents $F$ is then induced by assigning objects in $\Omega$ to the arguments of predicates in $\Psi$, i.e.~$F=\{p(\omega):p\in\Psi,\omega\in\Omega^{ar(p)}\}$ s.t. $\Omega^k$ is the $k$-th Cartesian power of $\Omega$.

Likewise, we assume that each action in $A$ is instantiated from an STRIPS operator schema. Figure~\ref{fig:stack} shows the {\em stack} operator schema for a STRIPS blocksworld represented in PDDL.
\begin{figure}[hbt]
\begin{footnotesize}
\begin{scriptsize}
\begin{verbatim}
(:action stack
  :parameters (?x1 ?x2)
  :precondition (and (holding ?x1) (clear ?x2))
  :effect (and (not (holding ?x1)) (not (clear ?x2))
               (clear ?x1) (handempty) (on ?x1 ?x2)))
\end{verbatim}
\end{scriptsize}
\end{footnotesize}
 \caption{\small Example of a {\em stack} operator schema for a STRIPS blocksworld represented in PDDL.}
\label{fig:stack}
\end{figure}

Let $\Omega_v$ be a new set of objects, $\Omega\cap\Omega_v=\emptyset$, that represent {\em variable names}. $|\Omega_v|$ is given by the action with the maximum arity in a planning frame. For instance, in a three-block blocksworld $\Omega=\{block_1, block_2, block_3\}$ and $\Omega_v=\{v_1, v_2\}$ because the operators {\small\tt stack} and {\small\tt unstack} are the ones with the maximum arity (two parameters each). 

Let us define a new set of fluents $F_{v}$ that results instantiating $\Psi$ but using only the {\em variable objects} $\Omega_v$. In the blocksworld $F_v$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$), clear($v_2$), ontable($v_1$), ontable($v_2$), on($v_1,v_1$),on($v_1,v_2$), on($v_2,v_1$),on($v_2,v_2$)\}}.

We are now ready to define a STRIPS operator schema as a tuple $\xi=\tup{head(\xi),pre(\xi),add(\xi),del(\xi)}$:
\begin{itemize}
\item $head(\xi)=\tup{name(\xi),pars(\xi)}$, represents an operator {\em header} defined by its corresponding action name and a list of variables, $pars(\xi)\in\Omega_v^{ar(\xi)}$. The headers for the blocksworld operators are {\small\tt pickup($v_1$), putdown($v_1$), stack($v_1,v_2$)} and {\small\tt unstack($v_1,v_2$)}.
\item The preconditions $pre(\xi)\subseteq F_v$, the positive effects $add(\xi)\subseteq F_v$, and the negative effects $del(\xi)\subseteq F_v$ such that, $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$.
\end{itemize}


\subsection{Classical planning with conditional effects}
Our approach for leaning STRIPS action models is compiling this leaning task into a classical planning task with conditional effects. We use conditional effects because they allow us to compactly define actions whose effects depend on the current state. Many classical planners cope with conditional effects without compiling them away. In fact, the support of PDDL conditional effects was a requirement for participating at IPC-2014~\cite{vallati:IPC:AIM2015}.

Now an action $a\in A$ has a set of literals $\pre(a)\in\mathcal{L}(F)$ called the {\em precondition} and a set of conditional effects $\cond(a)$. Each conditional effect $C\rhd E\in\cond(a)$ is composed of two sets of literals $C\in\mathcal{L}(F)$ (the condition) and $E\in\mathcal{L}(F)$ (the effect).

An action $a\in A$ is applicable in state $s$ if and only if $\pre(a)\subseteq s$, and the resulting set of {\em triggered effects} is
\[
\eff(s,a)=\bigcup_{C\rhd E\in\cond(a),C\subseteq s} E,
\]
i.e.~effects whose conditions hold in $s$. The result of applying $a$ in $s$ is a new state $\theta(s,a)=(s\setminus \eff^-(s,a))\cup\eff^+(s,a)$, where $\eff^-(s,a)$ and $\eff^+(s,a)$ are the negative and positive effects in $\eff(s,a)$.


\section{Learning STRIPS action models}
Learning STRIPS action models from fully available input knowledge, i.e. a set of plans where the {\em pre-} and {\em post-states} of every action in a plan are available, is straightforward. In this case, the operators schema are derived lifting the literals that change between the pre and post-state of the corresponding action executions. Likewise, preconditions are derived lifting the minimal set of literals appearing in all the pre-states that correspond to the same operator.

This section formalizes more challenging tasks, for learning STRIPS action models, where less input knowledge is available. Next we formalize these learning tasks according to the available input knowledge.

\subsection{Learning from labeled plans}
This learning task is formalized as $\Lambda=\tup{\Psi,\Pi,\Sigma}$: 
\begin{itemize}
\item $\Psi$, the set of predicates that define the abstract state space of a given planning domain. This set includes the predicates for defining the headers of the operators schema.
\item $\Pi=\{\pi_1,\ldots,\pi_{\tau}\}$, the given set of example plans.
\item $\Sigma=\{\sigma_1,,\ldots,\sigma_{\tau}\}$, a set of labels s.t. each plan $\pi_t$, {\small $1\leq t\leq \tau$}, has a label $\sigma_t=(s_0^t,s_{n}^t)$ where $s_{n}^t$ is the state resulting from executing $\pi_t$ starting from the state $s_0^t$. 
\end{itemize}

A solution to the learning task $\Lambda$ is a set of operator schema $\Xi$ (one schema for each operator header) compliant with the predicates in $\Psi$, the example plans $\Pi$, and their labels $\Sigma$.

\subsection{Learning from initial/final states}
Here we reduce the amount of input knowledge provided to the learning task. Now $\Pi=\{\pi_1,\ldots,\pi_{\tau}\}$ is replaced by  $\Pi'=\{|\pi_1|,\ldots,|\pi_{\tau}|\}$ i.e. $\Pi'$ that does not contain a set of plans but the number of actions of each plan, so the learning task is redefined as $\Lambda'=\tup{\Psi,\Pi',\Sigma}$. While the previous learning task, $\Lambda$, corresponds to watching an agent acting in the world, this new learning task $\Lambda'$ can be understood as watching only the results of its actions executions knowing the number of diferent actions performed by the agent.

Finally, we can go a step further and redefine a third learning task $\Lambda''=\tup{\Psi,\Sigma}$ that corresponds to watching only the results of the plan executions. In this case a solution to the $\Lambda''$ learning task is a set of operator schema $\Xi$ that is compliant only with the predicates in $\Psi$, and the given set of initial and final states $\Sigma$.

In these two cases, a solution must not only synthesize the action models but also the actions that produced the given labels (this information is no longer given in the learning examples).


\section{Learning STRIPS action models with classical planning}
Our approach for addressing a learning task $\Lambda$, $\Lambda'$ or $\Lambda''$, is to compile it into a classical planning task $P_{\Lambda}$, $P_{\Lambda'}$ or $P_{\Lambda''}$. The intuition behind these compilations is that a solution to the resulting classical planning task is a sequence of actions that:
\begin{enumerate}
\item Programs the STRIPS action model. For each $\xi\in\Xi$, determines the literals that belong to the sets $pre(\xi)$, $del(\xi)$ and $add(\xi)$.
\item Validates the programmed action model with the given set of labels $\Sigma=\{\sigma_1,\ldots,\sigma_{\tau}\}$. For every {\small $1\leq t\leq \tau$}, the programmed action model $\Xi$ is used to produce the final states $s_{n}^t$ starting from their corresponding initial state $s_0^t$.
\end{enumerate}

To formalize these compilations we first define {\small $1\leq t\leq \tau$} classical planning instances $P_t=\tup{F,\emptyset,I_t,G_t}$, that belong to the same planning frame (i.e. share the same fluents and actions and differ only in the initial state and goals). The set of fluents $F$ is built instantiating the predicates in $\Psi$ with the objects appearing in the labels. Formally $\Omega=\{o|o\in s_0^t\cup s_n^t\cup\pi_t, {\small 1\leq t\leq \tau}\}$. The set of actions is empty $A=\emptyset$, this is the aim of the learning tasks adressed in the paper. Finally the initial state $I_t$ is given by the state $s_0^t\in \sigma_t$ while goals $G_t$, are defined by the state $s_n^t\in \sigma_t$. 

Now we are ready to formalize our compilations for learning STRIPS action models using classical planning with conditional effects. We start with $\Lambda''$ that is the learning task with the minimum amount of input knowledge and incrementally extend the formmilized compilation until addressing $\Lambda$. Given a learning task $\Lambda''=\tup{\Psi,\Sigma}$ the compilation outputs a classical planning task $P_{\Lambda''}=\tup{F_{\Lambda},A_{\Lambda},I_{\Lambda},G_{\Lambda}}$:
\begin{itemize}
\item $F_{\Lambda}$ is an extension of $F$ with:
\begin{itemize}
\item Fluents representing the programmed action model: $pre_f(\xi)$, $del_f(\xi)$ and $add_f(\xi)$ for every $f\in F_v$ and $\xi \in \Xi$. If a fluent $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ holds, it means that $f$ is a precondition/negative effect/positive effect of the operator $\xi$. 
\item Fluents $\{test_t\}_{1\leq t\leq \tau}$, indicating the example where the programmed model is currently being validated.
\item Fluents $mode_{pre}$, $mode_{eff}$ and $mode_{val}$ indicating whether the preconditions of the action schema are being programmed, the effects of the action schema are being programmed or the programmed action models are being validated.
\end{itemize}
\item $I_{\Lambda}$, contains the fluents from $F$ that encode $s_0^1$, every fluent $pre_f(\xi)\in F_{\Lambda}$ (initially all operators have all the possible preconditions) and $mode_{pre}$. 
\item $G_{\Lambda}=\{test_t\}$,{\small $1\leq t\leq \tau$}, indicates that the programmed action model is validated in all the learning examples.
\item $A_{\Lambda}$ contains actions of three types:
\begin{enumerate}
\item The actions for programming the operator schema. This includes the actions for removing a {\em precondition} $f\in F_v$ from the action schema $\xi\in\Xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programPre_{f,\xi}})=&\{pre_{f}(\xi),mode_{pre}, \\
&\neg mode_{eff}, \neg mode_{val}\},\\    
\cond(\mathsf{programPre_{f,\xi}})=&\{\emptyset\}\rhd\{\neg pre_{f}(\xi)\}.
\end{align*}
\end{small}

Actions for adding a {\em negative} or {\em positive} effect $f\in F_v$ to the action schema $\xi\in\Xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programEff_{f,\xi}})=&\{\neg del_{f}(\xi),\neg add_{f}(\xi),\\                                                   
&\neg mode_{val}\},\\
\cond(\mathsf{programEff_{f,\xi}})=&\{pre_{f}(\xi)\}\rhd\{del_{f}(\xi)\},\\
&\{\neg pre_{f}(\xi)\}\rhd\{add_{f}(\xi)\},\\
&\{mode_{pre}\}\rhd\{\neg mode_{pre},mode_{eff}\}.
\end{align*}
\end{small}

\item The actions for applying an already programmed operator schema $\xi\in\Xi$ bounding it with objects $\omega\subseteq\Omega^{ar(\xi)}$
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{apply_{\xi,\omega}})=&\{pre_{f}(\xi)\implies p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{del_{f}(\xi)\}\rhd\{\neg p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{add_{f}(\xi)\}\rhd\{p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{mode_{pre}\}\rhd\{\neg mode_{pre},mode_{val}\},\\
&\{mode_{eff}\}\rhd\{\neg mode_{eff},mode_{val}\}.
\end{align*}
\end{small}

For instance, these actions define that if an operator is programmed with the precondition $holding(v_1)\in F_v$ it {\em implies} ($\implies$) that $holding(block_1)\in F$ has to be true in the current state if the operator binds the variable object $v_1\in\Omega_v$ with the regular object $block_1\in\Omega$. The operator binding is done implicitly, i.e. variables in $pars(\xi)$ are bound to the objects in $\omega$ appearing in the same position. 

\item The actions for changing the learning example {\small $1\leq t\leq \tau$} where the programmed action model is validated. 
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{t}})=&G_t\cup\{test_j\}_{j\in 1\leq j<t}\cup\{mode_{val}\},\\
\cond(\mathsf{validate_{t}})=&\{\emptyset\}\rhd\{test_t\}.
\end{align*}
\end{small}
\end{enumerate}
\end{itemize}


\begin{lemma}
Any classical plan $\pi$ that solves $P_{\Lambda}$ induces a valid action model that solves the learning task $\Lambda$.
\end{lemma}

\begin{proof}[Proof sketch]
Once the preconditions of an operator schema $\Xi$ are programmed they cannot be modified. The same happens with the positive and negative effects (that can only be programmed after all the preconditions are programmed). Furthermore, once an operator schema is programmed it can only be applied. The only way of achieving a fluent $\{test_t\}$,{\small $1\leq t\leq \tau$} is by executing an applicable sequence of programmed operator schema that achieves the goal state defined by its associated label $\sigma_t$ starting from the initial state of the corresponding label. If this is done for all the labels (all the input examples of the learning task) it means that the programmed action model $\Xi$ is compliant with the learning input knowledge and hence, it is a solution to the action model learning task.
\end{proof}

Interestingly, the compilation is valid for partially specified action models since known preconditions and effects (fluents $pre_f(\xi)$, $del_f(\xi)$ and $add_f(\xi)$) can be part of the initial state $I_{\Lambda}$ and the corresponding programming actions ($\mathsf{programPre_{f,\xi}}$ and $\mathsf{programEff_{f,\xi}}$) be removed from $A_{\Lambda}$ making the classical planning task $P_{\Lambda}$ easier. 

\section{Constraining the hypothesis space with example plans}
The compilation can be extended to the learning scenario defined by $\Lambda$ and $\Lambda'$ in which a set of plans $\Pi$ (or only its lengths in the case of $\Lambda'$) is available. Each plan $\pi_i\in \Pi$, {\small $1\leq i\leq t$}, is a solution to the corresponding classical planning instance $P_i=\tup{F,A,I_i,G_i}$ defined above. The compilation extensions are: 
\begin{itemize}
\item $F_{\Lambda}$ includes the new set of fluents $F_{\Pi}=\{plan(name(\xi),j,\Omega^{ar(\xi)})\}$ for encoding the $j$ steps of the {\small $1\leq i\leq t$} plans in $\Pi$ with $F_{\Pi_i}\subseteq F_{\Pi}$ the fluents encoding the plan corresponding to the $i^{th}$ example (only for the $\Lambda$ case). In addition fluents $at_j$ and $next_{j,j_2}$, {\small $1\leq j<j2\leq n$}, represent the plan step where the programmed action model is validated ($n$ is the max length of a plan in $\Pi$).
\item $I_{\Lambda}$ is extended with the fluents from $F_{\Pi_1}$ that encode the plan $\pi_1\in \Pi$ for solving $P_1$, and the fluents $at_1$ and $\{next_{j,j_2}\}$, {\small $1\leq j<j2\leq n$}, for indicating the plan step where to start validating the programmed action model. Goals $G_{\Lambda}$ are like in the original compilation.
\item With respect to the actions in $A_{\Lambda}$,
\begin{enumerate}
\item The actions for programming the preconditions/effects of a given operator schema are the same.
\item The actions for applying an operator schema have an extra precondition $f\in F_{\Pi_i}$ that encodes the current plan step and extra conditional effect $\{at_{j}\}\rhd\{\neg at_{j},at_{j+1}\}_{\forall j\in [1,n]}$ for advancing the plan step.
\item The actions for changing the active test have an extra precondition, $at_{|\Pi_i|}$, to indicate that we simulated the full current plan $\Pi_i$ and extra conditional effects to load the next plan $\Pi_{i+1}$ where to validate the programmed action model:
\begin{small}
\begin{align*}
&\{f\}\rhd\{\neg f\}_{f\in F_{\Pi_i}},\\
&\{\emptyset\}\rhd\{f\}_{f\in F_{\Pi_i+1}},\\
&\{\emptyset\}\rhd\{\neg at_{|\pi_i|},at_1\}.
\end{align*}
\end{small}
\end{enumerate}
\end{itemize}


\section{Evaluation}

\subsection{Learning action models from example plans}
\begin{table*}[!hbt]
\begin{footnotesize}
\begin{tabular}{l||c|c|c||c|c|c||c|c|c}


\end{tabular}
\end{footnotesize}
\caption{\small Mean error and standard deviation of the learned models when using hand-picked examples and examples collected using the classical planner Fast-Downward.}
  \label{tab:eplans}
\end{table*}

The performance of our learning approach is evaluated for different degrees of available input knowledge and using different sources for collecting this input knowledge. In all the cases we assess the performance of our learning approach using the cardinality of the {\em symmetric difference} sets that are computed between the set of preconditions, del and add effects (1), in the learned model and (2), in the actual models. In all the experiments the compilation is solved using the SAT-based classical planner {\sc Madagascar}~\cite{rintanen2014madagascar}.

Table~\ref{tab:eplans} shows the mean error and standard deviation of the learned models with respect to the actual action models when (1) using {\em hand-picked} examples, (2) examples collected using the classical {\em planner} {\sc Fast-Downward}~\cite{helmert2006fast} and (3) examples collected {\em randomly}. The standard deviation provides a measure of how this error is distributed among the different operators in the domain. If this deviation is 0 it means that is equally distributed in all the domain operators.


\subsection{Learning action models from example states}
TBD.


\section{Related work}

%The  LIVE  system  (Shen  and  Simon,  1989)  was  an  extension  of  the  General  Problem  Solver  (GPS) framework (Ernst and Newell, 1969) with a learning component. LIVE alternated problem solving with model  learning  to  automatically  define  operators.  The  decision  about  when  to  alternate  depended  onsurprises,  that  is  situations  where  an  action  effects  violated  its  predicted  model.  EXPO  (Gil,  1992) generated plans with the  PRODIGY  system (Minton, 1988), monitored the plans execution, detected differences in the predicted and the observed states and constructed a set of specific hypotheses to fix those differences. Then the  EXPO  filtered the hypotheses heuristically.  OBSERVER  (Wang, 1994) learned operators by monitoring expert agents and applying the version spaces algorithm (Mitchell, 1997) to the observations. When the system already had an operator representation, the preconditions were updated by removing facts that were not present in the new observation’s pre-state; the effects were augmented by adding facts that were in the observation’s delta-state. All of these early works were based on direct liftings of the observed states. They also benefit from experience beyond simple interaction with the environment such as exploratory plans or external teachers, but  none  provided  a  theoretical  justification  for  this  second  source  of  knowledge.  The  work  recently reported in (Walsh and Littman, 2008) succeeds in bounding the number of interactions the learner must complete to learn the preconditions and effects of a STRIPS action model. This work shows that learning STRIPS  operators  from  pure  interaction  with  the  environment,  can  require  an  exponential  number  of samples, but that limiting the size of the precondition lists enable sample-efficient learning (polynomial in the number of actions and predicates of the domain). The work also proves that efficient learning is also possible without this limit if an agent has access to an external teacher that can provide solution traces on demand.

%Others systems have tried to learn more expressive action models for deterministic planning in fully observable environments. Examples would include the learning of conditional costs for AP actions (Jess Lanchas and Borrajo, 2007) or the learning of conditional effects with quantifiers (Zhuo et al., 2008).

%In addition action model learning has been studied in domains where there is partial state observability. ARMS uses the same kind od learning examples but assumes the exampls are given and proceeds in two phases. In the first phase, ARMS extracts frequent action sets from plans that share a common set of parameters. ARMS also finds some frequent literal-action pairs with the help of the initial state and the goal state that provide an initial guess on the actions preconditions, and effects. In the second phase, ARMS uses the frequent action sets and literal-action pairs to define a set of weighted constraints that must hold in order to make the plans correct. Then, ARMS solves the resulting weighted MAX-SAT problem and produces action models from the solution of the SAT problem. This process iterates until all actions are modelled. For a complex planning domain that involves hundreds of literals and actions, the corresponding weighted MAX-SAT representation is likely to be too large to be solved efficiently as the number of clauses can reach up to tens of thousands. For that reason ARMS implements a hill-climbing method that models the actions approximately. Consequently, the ARMS output is a model which may be inconsistent with the examples.

%(Amir and Chang, 2008) introduced an algorithm that tractably generates all the STRIPS-like models that  could  have  lead  to  a  set  of  observations.  Given  a  formula  representing  the  initial  belief  state,  a sequence of executed actions and the corresponding observed states(where partial observations of states are given), it builds a complete explanation of observations by models of actions through a Conjunctive Normal Form (CNF) formula. By linking the possible states of fluents to the effect propositions in the action models, the complexity of the CNF encoding can be controlled to find  exact  solutions  efficiently  in  some  circumstances.  The  learning  algorithm  updates  the  formula  of the belief state with every action and observation in the sequence. This update makes sure that the new formula represents all the transition relations consistent with the actions and observations. The formula returned at the end includes all consistent models, which can then be retrieved with additional processing. Unlike the previous approaches, the one described in (Mour ao et al., 2008) deals with both missing and noisy predicates in the observations. For each action in a given domain, they use kernel perceptrons to learn predictions of the domain properties that change because of the action execution. LOCM (Cresswellet al., 2009) induces action schemas without being provided with any information about initial, goal or intermediate state descriptions for the example action sequences. LOCM receives descriptions of plans or plan fragments, uses them to create states machines for the different domain objects and extracts the action schemas from these state machines.





\section{Conclusions}
This paper presents a novel approach for learning classical planning action models from minimal input knowledge and using exclusively existing classical planners. Learning action models from examples allows the reformulation of a domain theory. An interesting research direction is the study of domain reformulation using features that allow more compact solutions like the {\em reachable} or {\em movable} features in the Sokoban domain.
The size of the compilation output depends also on the number of examples. Empirical results show that our approach is able to generate non-trivial CFGs from very small data sets.

Last but not least, collecting {\em informative} examples for learning planning action models is challenging. Planning actions include preconditions that are only satisfied by specific sequences of actions, and often, with a low probability of being chosen by chance~\cite{fern2004learning}. In addition, motivated by the success of recent algorithms for exploring planning tasks~\cite{geffner:pwithsimulators:IJCAI17}, we do not assume that a learning set of plans is given apriori but instead, we autonomously collect the learning examples.


\bibliographystyle{aaai}
\bibliography{strips-learning}
\end{document}
